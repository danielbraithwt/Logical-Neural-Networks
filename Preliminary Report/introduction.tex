\chapter{Introduction}\label{C:intro}
Neural Networks (NN's) are commonly used to model supervised learning problems. NN's often achieve higher accuracy than other methods because they are able to approximate any continuous function. A well trained NN can generalize well but it is very difficult to interpret how the network is operating. This is called the black-box problem. \\

There are a number of motivations for wanting to solve the black-box problem. If a NN is able to provide an explanation for its output a deeper understanding of the problem can be developed, the rules or patterns learnt by an NN could represent some knowledge in the data which has not yet been identified. Another possibility is that the NN is being implemented to operate a critical systems which involves the safety of humans, a situation which is becoming more common place. In the context of safety critical systems being able to inspect the NN is a necessary part of ensuring the system is safe, because a NN could take potentially dangerous actions for situations where the action must be extrapolated \cite{andrews1995survey}.\\

By restricting the function set that each neuron can perform is it possible to create a more interpretable network? Restricted the functions for each neuron to be the function set taking some subset of inputs and perform a pre determined logical function, after training to identify the function each neuron is performing on its inputs only the subset of inputs considered must be identified as the operation is fixed.\\

This report develops a class of networks in which the function space of each neuron is restricted to be a predefined operation on a subset of its inputs, ideally it would be possible to consider this operation as a Boolean function, consequently each neuron can be interpreted as logical operation on its inputs. Boolean functions are by nature discrete, as such do not have a continuous differential, making them unsuitable for training with an algorithm such as Backpropagation. Instead of using discrete boolean functions this report makes use of Noisy-OR and Noisy-AND neurons \cite{LearningLogicalActivations}, which are generalized continuous parametrisations of OR and AND gates, meaning neurons are restricted to performing operations on a subset of their inputs which correspond to a logical OR or AND. Noisy neurons are placed in specific configurations which allow learning Conjunctive or Disjunctive Normal Form expressions, they are called Logical Normal Form Networks (LNFN's).\\

By the design of LNFN's it is possible to extract logical rules from the network once trained, by inspecting the weights of each Noisy neuron it is possible to determine the relationship between the inputs and output. Rule extraction algorithms all ready exist as a method to extract knowledge from NN's \cite{andrews1995survey}, some aim to discover rules which replace the NN and others extract rules which are combined with the NN to improve performance. Rule extraction algorithms are generally split into three categories. The \textbf{Decompositional Approach} extracts rules by analysing the activations and weights in the hidden layers. The \textbf{Pedagogical Approach} works by creating a mapping of the relationship between inputs and outputs. Finally The \textbf{Eclectic Approach} combines the previous two approaches \cite{andrews1995survey}.\\

The LNFN's developed in this report present an intuitive eclectic method for extracting rules from such a network. The restriction placed on the function space of each neuron, while improving the interpretability, intuitively will also hinder their ability to be universal approximators. Along with the development of a new rule extraction approach this report will identify and explore the issues introduced by the restriction placed on neurons to determine where the rule extraction algorithm can be used.\\

When provided a truth table for a boolean expression, the performance of LNFN's has no statistically significant differences to that of a Multi-Layer Perceptron (MLPN) Network. The LNFN's are also able to generalize, obtaining statistically equal performances as a MLPN when given incomplete truth tables.\\

