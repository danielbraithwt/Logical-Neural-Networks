%% $RCSfile: proj_proposal.tex,v $
%% $Revision: 1.3 $
%% $Date: 2016/06/10 03:44:08 $
%% $Author: kevin $

\documentclass[11pt, a4paper, twoside, openright]{report}

\usepackage{float} % lets you have non-floating floats

\usepackage{url} % for typesetting urls

%  We don't want figures to float so we define
%
\newfloat{fig}{thp}{lof}[chapter]
\floatname{fig}{Figure}

%% These are standard LaTeX definitions for the document
%%
\title{Logical Neural Networks: Opening the black box}
\author{Daniel Thomas Braithwaite}

%% This file can be used for creating a wide range of reports
%%  across various Schools
%%
%% Set up some things, mostly for the front page, for your specific document
%
% Current options are:
% [ecs|msor|sms]          Which school you are in.
%                         (msor option retained for reproducing old data)
% [bschonscomp|mcompsci]  Which degree you are doing
%                          You can also specify any other degree by name
%                          (see below)
% [font|image]            Use a font or an image for the VUW logo
%                          The font option will only work on ECS systems
%
\usepackage[image,ecs,bschonscomp]{vuwproject} 

% You should specifiy your supervisor here with
%     \supervisor{Firstname Lastname}
% use \supervisors if there are more than one supervisor
\supervisor{Marcus Frean}

% Unless you've used the bschonscomp or mcompsci
%  options above use
%   \otherdegree{OTHER DEGREE OR DIPLOMA NAME}
% here to specify degree

% Comment this out if you want the date printed.
\date{}

\begin{document}

% Make the page numbering roman, until after the contents, etc.
\frontmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Neural networks have been shown to be universal function approximators  and have been employeed to solve a number of non-trivial problems. The downside to these networks is that they are very diffcult (if not impossible) to understand. We aim to show that building NN's out of Noisy logic gates are also universal function approximators, have comparable peformance to regular NN's and the added benifit of being understandable to humans
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle

%\tableofcontents

% we want a list of the figures we defined
%\listof{fig}{Figures}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{1. Introduction}

Neural Networks peform exceptonallty well over a wide range of different problems. However once trained these networks become a black box, near impossible for a human to understand what features the network is using to solve the problem presented to it. Logical Neural Netowrks (neural networks with logical activation functions) have been shown to provide a more understandable representation, however LLN's have been neglected not alot of effort has been put into developing them.

\section*{2. The Problem}

Research done previously into LLN's has shown that using a network with layers consisting of Noisy-OR and Noisy-And neurons can achieve a reasonable accuracy on the MINST dataset while also providing a more understandable network. To begin this project we take a step back to ensure we have the right approach. A network built out of neurons that resembel logic gates we would expect would be able to learn any given boolean formula, if we cant achieve this then there is a fundimental problem with our approach to LLN's.\\

Ideally we would give a mathematical proof showing that LLN's are (or are not) universal boolean function approximators. If LLN's are this would provide confidence for them also being universal function approximators in the general case. If we arnt able to prove (or provide evedence via experements) that LLN's are universal function approximators then there uses would be very limited.

\section*{3. Proposed Solution}

Before we can start we must address a fundimental issue with our exsisting setup.  Namely our choice of "gates" is not a functionally complete set. Trying to learn a boolean function with only AND and OR might work but it will severly limit us. We proposes to implement a number of different sets of functionally complete neurons, such as NAND, NOR. Ideally we would find that they all give the same peformance but given we are modeling our descrete gates with a continous function we could see unexpected results. \\

After experementing with LLN's for approximating boolean functions we would have a good idea for wether they are universal boolean function approximators or not, but intuatively it would make sense for them to be, given that we would be using a functonally complete set of logic gates (which we know in there descrete form can represent any boolean expression). However as disussed before, given we are approximating our descrete gates as continues the same principles may not hold, for this reason we would first need to investigate what effects our continous model has on the propertys of logic gates.

\section*{4. Evaluating your Solution}

\section*{5. Resource Requirements}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\backmatter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bibliographystyle{ieeetr}
\bibliographystyle{acm}
\bibliography{sample}
\end{document}
