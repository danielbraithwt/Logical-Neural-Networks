%% $RCSfile: proj_proposal.tex,v $
%% $Revision: 1.3 $
%% $Date: 2016/06/10 03:44:08 $
%% $Author: kevin $

\documentclass[11pt, a4paper, twoside, openright]{report}

\usepackage{float} % lets you have non-floating floats

\usepackage{url} % for typesetting urls

%  We don't want figures to float so we define
%
\newfloat{fig}{thp}{lof}[chapter]
\floatname{fig}{Figure}

%% These are standard LaTeX definitions for the document
%%
\title{Logical Neural Networks: Opening the black box}
\author{Daniel Thomas Braithwaite}

%% This file can be used for creating a wide range of reports
%%  across various Schools
%%
%% Set up some things, mostly for the front page, for your specific document
%
% Current options are:
% [ecs|msor|sms]          Which school you are in.
%                         (msor option retained for reproducing old data)
% [bschonscomp|mcompsci]  Which degree you are doing
%                          You can also specify any other degree by name
%                          (see below)
% [font|image]            Use a font or an image for the VUW logo
%                          The font option will only work on ECS systems
%
\usepackage[image,ecs,bschonscomp]{vuwproject} 

% You should specifiy your supervisor here with
%     \supervisor{Firstname Lastname}
% use \supervisors if there are more than one supervisor
\supervisor{Marcus Frean}

% Unless you've used the bschonscomp or mcompsci
%  options above use
%   \otherdegree{OTHER DEGREE OR DIPLOMA NAME}
% here to specify degree

% Comment this out if you want the date printed.
\date{}

\begin{document}

% Make the page numbering roman, until after the contents, etc.
\frontmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstract}
Neural networks have been shown to be universal function approximators  and have been employed to solve a number of non-trivial problems. The downside to these networks is that they are a black box, incredibly difficult (if not impossible) to understand how the NN is operating once trained. Previous research showing that Logical Neural Networks applied to the MINST data set having comparable performance to standard NN's and a more understandable network. We aim to build on this by formally proving a foundation for LLN's, investigating where LLN's are best applied and whether we can approximate our LLN as a discrete logic circuit.
\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\maketitle

%\tableofcontents

% we want a list of the figures we defined
%\listof{fig}{Figures}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\mainmatter

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{1. Introduction}

Neural Networks (NN's) perform exceptionally well over a wide range of different problems. However once trained these networks become a black box, near impossible for a human to understand what features the network is using to solve the problem presented to it. Logical Neural Networks (LLN's), that is NN's with logic based activation functions, have previously [REFERENCE PREV HONORS PROJECT] been shown to provide a more understandable representation while still performing well (in the context of the MINST data set). Despite the promise of LLN's not a lot investigation into them has occurred.

\section*{2. The Problem}

LLN's have been shown to have  promise as an alternative to standard NN's with the added benefit of having sparse weights and being easier to understand. We suggest this problem still hasn't been completely solved as we have yet to develop a rigorous and formal foundation for LLN's. First we ask, can it be proven that like standard NN's, LLN's are also universal function approximators?  Before we can hope to answer this key question we also must ask whether we have the right parametrization of noisy gates as these form the foundation of an LLN. \\

Regardless of our answer to the first question a next logical step for our investigation is to ask, what types of problems is an LLN best suited to? Which ones can it not only perform well on but also provide a useful learned representation. This will help build the foundation of LLN's, giving us a better understanding of how to configure LLN's for different problems, i.e. network layout and which logical neurons to use.\\

Finally we pose a concept which LLN's seem uniquely qualified to solve given they are based of logic gates. Being able to create hardware NN's would be incredibly powerful (as hardware is alot faster). There doesn't seem to be any intuitive way to achieve this with standard NN's, however approximating the continuous logic gates in our LLN as discrete ones would make it possible. But is it possible to create an approximation in a feasible way without sacrificing to much accuracy?

\section*{3. Proposed Solution}

To begin this project we take a step back to ensure we have the right approach. A network built out of neurons that resemble logic gates we would expect should be able to learn any given boolean formula. This would show we have good paramaterisations for our noisy gates and be a fundamental step towards proving LLN's are universal function approximators. First we must address a key issue with our existing setup, namely our choice of "gates" is not a functionally complete set. So far LLN's have been shown to peform with a reasonable accuracy on the MINST data set using layers consisting of only Noisy-OR and Noisy-AND gates. Consider the issue of representing some know boolean expression using discrete logic circuit, but we only have access to AND and OR gates, AND combined with OR is not functionally complete. We propose to implement a number of different sets of functionally complete neurons, such as NAND and NOR. These logical neurons would be paramaterised so we can observe the same nice property's we see with discrete gates, i.e. NOT(AND) = NAND. If an LLN consisted of multiple layers of NAND's or NOR's we would expect to find groups of gates learning to act like some of our other gates. \\

The first 4 weeks of this project would be taken to fully understand the concept of Noisy gates, using the bibliography of the previous research and other resources. Along with developing our paramaterisation of logical gates. The gates we aim to paramaterise and implement are AND, OR, NOT, NAND, NOR. \\

Our choice of gates have paramaterisations which maintain convenient property's and are functionally complete, so intuitively we would expect our LLN to be able to learn any known boolean function, the results here will be interesting as we might find that theoretically our LLN can approximate any boolean function but in practice some can not be learned so easily in practice. As part of testing our LLN's on learning known boolean functions it would be interesting to experiment with the training data presented to the network, how can we train a network which generalizes well? Say we trained a LLN with one of the boolean inputs fixed at 0, then would the network be able to generalize to the case where that input is 1? We expect to spend 6 weeks investigating at the conclusion of which we aim to have a proof demonstrating that LLN's can approximate any know boolean function along with strong evidence showing which collections of neurons work well together, give interpretable representation in trained networks and in what configuration yields the best results. \\

Next we address a fundamental question, are LLN's universal function approximators? Working towards a proof or strong evidence showing this in ether direction we can start by experimenting with an LLN learning some general functions and comparing its performance to a standard NN. These experiments are not equivalent to a proof but will reveal which direction we should peruse. At the conclusion of this part of the project we will answer this with a proof or strong evidence, we allocate 6 weeks for this section. \\

Now that we have explored the foundation of LLN's, using this knowledge we want to use our LLN to solve standard benchmark problems and compare against performance of standard NN's. We would use K-Fold Cross Validation, which is commonly used for comparisons like this. It would then be useful to examine the trained LLN and see if we can understand how the network has learned to represent the problem as this really is the motivation behind LLN's. We allocate 4 weeks for this. \\

Seeing we have essentially constructed NN's out of neurons which resemble continuous logic gates, something to explore is can we feasibly approximate these continuous gates with discrete ones, essentially allowing us to create hardware NN's. Performing such a task would certainly require more gates than neurons but would this blowup result in something that could feasibly be implemented on a circuit board. We allocate any remaining time to investigating this problem and developing/implementing methods to solve it.\\

An overview of the proposed solution follows
\begin{itemize}
\item Produce bibliography and paramaterisation of gates (est: 4 weeks)
\item Proving LLN's can/can not approximate any known boolean function (est: 6 weeks)
\item Proving LLN's are/are not universal function approximators (est: 6 weeks)
\item Evaluate LLN's on benchmark problems (est: 4 weeks)
\item Explore approximating LLN as discrete logic circuit (est: remaining time)
\end{itemize}
This ends up allocating a large portion of time to the last task, however this accounts for falling behind schedule in some sections.


\section*{4. Evaluating your Solution}

A solution to this problem should build the foundation to LLN's. Giving a proof that LLN's can approximate any known boolean function and strong evidence/proof showing that LLN's are universal function approximators. This would require an in depth investigation into various paramaterisations of logic gates. Following this it should identify where LLN's are most powerful and where a standard NN should be used instead. Finally a study demonstrating ways to approximate an LLN as a logic circuit, commenting on the feasibility and accuracy of these methods.\\

Assuming we are able to show (or give strong evidence) LLN's are universal function approximators then also providing a comparason between NN's and LLN's on a number of benchmark problems .

\section*{5. Resource Requirements}

I do not for see any requirements I don't already have access to.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\backmatter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\bibliographystyle{ieeetr}
\bibliographystyle{acm}
\bibliography{sample}
\end{document}
