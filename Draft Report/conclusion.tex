\chapter{Conclusion}\label{C:con}
The growing number of situations where Artificial Neural Networks (ANNs) are used is driving the development of interpretable models. Being able to defend an ANNs decision can protect users from unsafe or ethically biased systems and protect companies utilizing such systems from breaching EU regulations.\\

A study of Logical Normal Form Networks developed algorithms initialize network weights (leading to good learning conditions) and extract rules from trained models. Through experimentation, such networks were demonstrated to have statistically equivalent performance and generalization to Multi-Layer Perceptron Networks. Training LNFNs on the Lenses and Iris datasets demonstrated their ability to learn multi-class classification problems and give insight into the data from their interpretable trained representation.\\

The foundational work developed the tools to derive modifications to the LNN structure giving them statistically equivalent performance to standard Multi-Layer Perceptron Networks and improving the interpretability of the learned models. Consequently, this report has shown that LNNs are a suitable alternative to Multi-Layer Perceptron Networks (of an equivalent size), obtaining a simpler and more interpretable model without sacrificing accuracy. By observing the weights of LNNs trained over the MNIST dataset, it was possible to determine what input features contributed to each classification and verified that the logic learned was sensible. LNNs where also shown to have comparable performance to recently developed network architectures which take a similar approach to what was done here. Beyond applications to classification tasks Logical Neural Networks where applied to Autoencoders. While the performance of Logical Autoencoders was worse than standard Sigmoid Autoencoders, it demonstrates the flexibility of LNNs and the range of situations where they can be applied.\\

Perhaps the most significant limitation of LNNs comes down to interpreting multi-layer logical neural networks. Because of this limitation, any future work which can develop better ways to interpret these models would increase the power of LNNs. These future studies might also focus on establishing the model interpretability more scientifically, on a larger number of problem domains and individuals who have the domain knowledge to assess how interpretable the model is.\\

This report has provided a formal foundation for Logical Neural Networks and shown their ability not only to learn accurate but also interpretable models. Limitations of LNNs have been identified, but this does not diminish their potential, but instead, provides avenues to increase their application.




