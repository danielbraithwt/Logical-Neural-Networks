\chapter{Conclusion}\label{C:con}
The growing number of situations where Artificial Neural Networks (ANNs) are used is driving the development of interpretable models. Being able to defend an ANNs decision can protect users from un safe or ethically biased systems and protect companies utilizing such systems from breaching EU regulations.\\

A study of Logical Normal Form Networks developed algorithms initialize network weights (leading to good learning conditions) and extract rules from trained models. Through experimentation such networks where demonstrated to have statistically equivalent performance and generalization to Multi-Layer Perceptron Networks. Training LNFNs on the Lenses and Iris data sets demonstrated their ability to learn multi class classification problems and give insight into the data from their interpretable trained representation.\\

The foundational work developed the tools to derive modifications for the LNN structure giving them statistically equivalent performance to standard Multi-Layer Perceptron Networks and improving the intepretability of the leant models. Consequently this report has shown that LNNs are a suitable alternative to Multi Layer Perceptron Networks, obtaining a simpler and more interpretable model without sacrificing accuracy. By observing the weights of LNNs trained over the MNIST data set it was possible to determine what input features contributed to each classification and verify that the logic learnt was sensible. LNNs where also shown to have comparable peformance to recently developed network archetchures which take a similar approach to what was done here. Beyond applications to classification tasks Logical Neural Networks where applied to Auto Encoders. While the performance of Logical Auto Encoders was worse than standard Sigmoid Auto Encoders it demonstrates the flexibility of LNNs and the range of situations where they can be applied.\\

Perhaps the biggest limitation of LNNs comes down to interpreting multi layer logical neural networks. Because of this limitation any future work which can develop better ways to interpret these models would increase the power of LNNs. These future studies might also focus on establishing the model intepreability in a more scientific manner. On a larger number of problem domains and individuals who have the domain knowledge to assess how interpretable the model is.\\

This report has provided a formal foundation for Logical Neural Networks and shown their ability to not only learn accurate but also interpretable models. While limitations have been identified this does not diminish potential of LNNs but rather provides avenues to increase their application.




