\chapter{Conclusion}\label{C:con}
The growing number of situations where Artifical Neural Networks (ANNs) are used is driving the development of intepretable models. Being able to defend an ANNs decision can protect users from un safe or ethically biased systems and protect companies utilizing such sysems from breaching EU regulations.\\

A study of Logical Normal Form Networks developed algorythms initilize network weights (leading to good learning conditons) and extract rules from trained models. Through experementation such networks where demonstraited to have stistically equivelent peformance and generalization to Multi-Layer Perceptron Networks. Training LNFNs on the Lenses and Iris data sets demonstraited their ability to learn multi class classification problems and give insite into data by its intepretable trained representaion.\\

The foundational work developed the tools to derive modifications for the LNN structure giving them stistically equivelent peformance to standard Multi-Layer Perceptron Networks and improving the intepretability of the leant models. Consequently this report has shown that LNNs are a sutable alternative to Multi Layer Perceptron Networks, obtaining a simpler and more intepretable model without scarfising acuracy. By obesrving the weights of LNNs trainined over the MNIST data set it was possible to determin what input features contributed to each classification and verify that the logic learnt was "sensible".\\

Beyond applications to classification tasks Logical Neural Networks where applied to Auto Encoders. While the peformance of Logical Auto Encoders was worse than standard Sigmoid Auto Encoders it demonstraits the flexability of LNNs and the range of situations where they can be applied.\\

Prehapse the biggest limitation of LNNs comes down to intepreting multi layer logical neural networks. It is possible to directly extract the influence each input feature has on the ouputs in LNNs with no hidden layers. However hidden layers introduce dependencies between input features making it diffcult to find a direct influence of inputs on the outputs. Because of this limitation any future work which can develop better ways to intepret these models would increase the power of LNNs.\\

Future studys might also focus on establishing the model intepreability in a more scienfic manner. On a larger number of problem domains and individuals who have the domain knowledge to assess how intepretable the model is.\\

This report has provided a formal foundation for Logical Neural Networks and shown their ability to not only learn accurate but also intepretable models. While limitations have been identified this does not deminish potential of LNNs but rather provides avenues to increase their application.




