\chapter{Conclusion}\label{C:con}

\comment{Talk about final applications here when they have been finished}

This report demonstrated that Logical Normal Form Networks (LNFNs) had statistically equivalent performance and generalization to Multi-Layer Perceptron Networks when trained over a number of randomly generated truth tables. LNFNs where shown to learn a representation which corresponded to a correct boolean formula, this formula was also able to generalize.\\

While LNFNs have these nice properties there are significant down sides which result in them being impractical. The critical issue is that they require $2^n$ hidden neurons.\\

A generalized version of LNFNs called Logical Neural Networks (LNNs) where introduced and improved apon to get a improvement in performance. LNNs where then shown to have more interpretable weight by training different network structures on the MNIST data set.\\

Finally ...
