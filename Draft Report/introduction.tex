\chapter{Introduction}\label{C:intro}
Artificial Neural Networks (ANN's) are commonly used to model supervised learning problems. NN's often achieve higher accuracy than other methods because they are able to approximate any continuous function. A well trained NN can generalize well but it is very difficult to interpret how the network is operating, this is called the black-box problem. \\

The growing number of situations in which ANN systems are used has lead to an increasing interest in systems which are not only accurate but also provide explanations of there answers \cite{doshi2017towards}. This increasing interest is driven by the verity of applications of ANNs where incorrect or biases answers can have significant effects on users.\\

Safety and ethics are two concrete examples of such situations where explanations of reasoning would provide a method to ensure an ANN can be implement with mitigated risk \cite{doshi2017towards}. In the context of Safety an ML system often can not be tested against all possible situations as it is computationally infeasible, the reasoning the ML system uses could be examined for flaws that could be result in a failure \textbf{Example Here}. On the other hand it is important to consider the implications of an ANN being biased towards a protected class of people, \textbf{Example Here}\\

Another pressure causing the development of this field is changing laws, in 2018 a European Union (EU) regulation will come into effect requiring algorithms which make decisions based on user level predictors must provide an explanation \cite{goodman2016european}. This EU regulation is heavily related to non-discrimination, and requires that measures be implemented which prevent discriminatory effects \cite{goodman2016european}.\\

\textbf{Explain Why Neural Networks Are Difficult To Interpret}\\

Consider some function $f$, then the operation of restricting a neuron to $f$ is given by only allowing a function given by $f$ operating on the set of weighted inputs (each input multiplied by a corresponding weight) to be learnt. By restricting the function set that each neuron can learn is it possible to create a more interpretable network? This report develops a class of networks where the function space of each neuron is restricted to be a predefined operation which can be interpreted as Boolean expression i.e. an AND or OR of its inputs.\\

Boolean functions are by nature discrete, as such do not have a continuous differential, making them unsuitable for training with an algorithm such as Backpropagation. This report makes use of Noisy-OR and Noisy-AND neurons \cite{LearningLogicalActivations}, which are generalized continuous parametrisations of OR and AND gates.\\

ANN's consisting of Noisy neurons are called Logical Neural Networks (LNN's). LNN's have been used to classify the MINST dataset with promising results, while not being able to achieve a state of the art accuracy they have been shown to yield simpler (sparser) and more interpretable weight vectors \cite{LearningLogicalActivations}. It was also shown that using Noisy neurons in combination with standard sigmoid units results in poor performing networks without the benefit of being more interpretable \cite{LearningLogicalActivations}, for this reason networks of this kind are not considered.\\

This report takes a different approach to using these interpretable Noisy neurons, instead they are placed in specific configurations which allow learning Conjunctive or Disjunctive Normal Form expressions, these are called Logical Normal Form Networks (LNFN's) and are a subset of LNN's.\\

This report demonstrates by experimentation that when provided a complete truth table for a boolean expression, the performance of LNFN's has no statistically significant differences to that of a Multi-Layer Perceptron (MLPN) Network. The LNFN's are also able to generalize, obtaining statistically equivalent performance to a MLPN when given incomplete truth tables.\\

By inspecting the weights of each Noisy neuron it is possible to determine a relationship between the inputs and output. Rule extraction algorithms exist as a method to extract knowledge from NN's \cite{andrews1995survey}, if LNFNs are more interpretable then are rules able to be extracted from them? When first considering problems with Boolean inputs and outputs, if a low enough error is achieved when training an LNFN over a complete truth table, it is possible to extract boolean rules from each neuron, consequently it is possible to obtain a boolean expression which represents the ANN and original truth table. Training over entire a complete data set is an unlikely scenario, this report investigates what effect training with incomplete truth tables has on any extracted formula. Only being able to apply LNFNs to Boolean problems does not make them very useful, this report investigates ways these networks can be applied to problems with inputs in a continuous domain.\\

The restriction placed on the function space of each neuron, while improving the interpretability, intuitively will also hinder their ability to be universal approximators. Along with the investigation of LNFNs and their potential, the limitations are also explored.

