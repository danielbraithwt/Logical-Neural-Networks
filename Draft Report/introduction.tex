\chapter{Introduction}\label{C:intro}

Artificial Neural Networks (ANN's) are commonly used to model supervised learning problems. \hl{A well trained ANN can generalize well, but it is difficult to interpret how the network is operating}. This issue with interpretation makes ANNs like a black-box. This report aims to alleviate this problem by formalizing and developing a novel neural network architecture that builds interpretability into its structure.

\section{Motivation}
\hl{The number of situations in which ANN systems are used is growing}, leading to an increasing interest in systems which are not only accurate but also provide a means to understand the logic used to derive their answers \cite{doshi2017towards}. Interest in interpretable systems is driven by the variety of situations that utilize ANNs where incorrect or biased answers can have significant effects on users.\\

Ensuring a system is Safe and Ethical are two concepts which depending on the application are important to verify. If an ANN was able to provide its reasoning for giving a specific output then defending actions made by the system would be a more feasible task \cite{doshi2017towards}.\\

In the context of safety, a Machine Learning (ML) system often cannot be tested against all possible situations, as it is computationally infeasible to do so. If accessible, the logic contained inside the model could be used to verify that the system will not take any potentially dangerous actions.\\

It is also important to consider the implications of an ANN which is biased against a protected class of people. A paper published in 2017 demonstrated that standard machine learning algorithms trained using text data learn stereotyped biases \cite{caliskan2017semantics}. An ANN designed with the intent to be fair could result in a system which discriminates because of implicit biases in the data used for training.\\

Another pressure causing the development of interpretable ML systems is changing laws. In 2018 the European Union (EU) General Data Protection Regulation \cite{eu-dgpr} (GDPR or "right to explanation") will come into effect. The GDPR will require algorithms which profile users based on their attributes be able to provide \textit{"Meaningful information about the logic involved"}. This law would affect many situations where ML systems are used. For example, banks, which use ML systems to make loan application decisions \cite{goodman2016european}. Using some simulated data sets, researchers trained an ANN to compute the probability of loan repayment \cite{goodman2016european}. The simulated data consisted of white and non-white individuals, both groups with the same proportion of the population that made repayments. As the proportion of white to non-white individuals in the data increased, the ANN became less likely to grant loans to non-white individuals. This artificial situation demonstrates the effect biased data could have on an ANN.\\

The argument thus far has established that being able to defend or verify the decisions made by ANNs is not just an interesting academic question. It would allow for the creation of potentially safer and fairer ML systems. They provide a means to verify that not only correct decisions are made, but they are made for justifiable reasons. The GDPR gives a monetary motivation to use interpretable systems as breaches of the regulation will incur fines \cite{goodman2016european}.\\

\section{Solution}
To address the problems laid out thus far we improved upon a probability based network architecture called Logical Neural Networks (LNNs) which yield a simpler trained model \cite{LearningLogicalActivations}.\\

Existing methods for interpreting ANNs are post-hoc algorithms to extract some meaning from standard ANN's. The approach presented in this report builds interpretability into the ANN through a structure based off logical functions. This report introduces a formal foundation for LNNs through the following stages.

\begin{enumerate}
	\item Motivate the concept that Logical Neural Networks are built from (Chapter \ref{C:backgroundsurvey}).
	\item Motivate and derive at a particular case of Logical Neural Networks called Logical Normal Form Networks (Chapter \ref{C:foundation-of-lnfns}).
	\item Discuss the performance, generalization and interpretation capabilities of Logical Normal Form Networks (Chapter \ref{C:foundation-of-lnfns}).
	\item Discuss the situations where Logical Normal Form Networks can be applied (Chapter \ref{C:investigation-of-lnfns})
	\item Generalise Logical Normal Form Networks to Logical Neural Networks (Chapter \ref{C:lnn}).
	\item Derive modifications to the Logical Neural Network architecture to improve accuracy (Chapter \ref{C:lnn})
	\item Evaluate the modified Logical Neural Network structure (Chapter \ref{C:evaluation-lnn})
	\item Demonstrate the possible use cases of Logical Neural Networks (Chapter \ref{C:lnn-application}).
\end{enumerate}

Following the development of Logical Neural Networks, they are evaluated on the MNIST database and compared against standard Multi-Layer Perceptron Networks. It will be demonstrated that LNNs are a reasonable alternative to standard MLPNs as they are simpler to interpret and do not sacrifice performance.