\chapter{Foundation of Logical Normal Form Networks}\label{C:foundation-of-lnfns}
Consider problems with a boolean expression describing the relation ship between binary inputs and outputs. This is certainly a restricted space of problems, however its a logical place to start if defining the activation functions of neurons as a "boolean like" function.\\

With this restriction in place any problem must be described by a boolean expression, this information alone does not help with constructing interpretable networks. It is known that any boolean expression has a CNF and DNF. It is possible to construct networks which can learn the underlying CNF or DNF, such networks are called Logical Normal Form Networks (LNFNs) \cite{herrmann1996backpropagation}. The research developing LNFNs has little justification for key decisions, consequently is difficult to understand and reproduce.\\

Using the idea of LNFNs \cite{herrmann1996backpropagation} and Pure Logical Neural Networks \cite{LearningLogicalActivations} new definitions are given for LNFNs in terms of Noisy-OR and Noisy-AND neurons.

\theoremstyle{definition}
\begin{definition}
A \textbf{CNF-Network} is a three layer network where neurons in the hidden layer consist solely of Noisy-OR's and the output layer is a single Noisy-AND. 
\end{definition}

\theoremstyle{definition}
\begin{definition}
A \textbf{DNF-Network} is a three layer network where neurons in the hidden layer consist solely of Noisy-AND's and the output layer is a single Noisy-OR. 
\end{definition}

\theoremstyle{definition}
\begin{definition}
A \textbf{LNF-Network} is a DNF or CNF Network
\label{def:lnfn}
\end{definition}

A CNF or DNF formula contains clauses of literals which is either an atom or a negation of an atom. To account for this the number of inputs to the network will be doubled, the inputs will be all the atoms and negations of the atoms, i.e. if $x_1, x_2$ are the atoms then $x_1, \lnot x_1, x_2, \lnot x_2$ are the inputs to the network.\\ 

It must also be determined how many hidden units the LNFN will have, it is known that $2^n$, n being the number of atoms, is an upper bound on the number of clauses needed in a CNF and DNF formula (see Theorem \ref{thm:max-clause-cnfdnf}).

\begin{theorem}
Let T be the complete truth table for the boolean formula B. Let L be an LNFN, if L has $2^n$ hidden units then there always exists a set of weights for L which correctly classifies any assignment of truth values to atoms.
\label{thm:upper-bound-hidden-units}
\end{theorem}

\begin{proof}
Let T be the truth table for a boolean function B. The atoms of B are $x_1, ..., x_n$. T has exactly $2^n$ rows. Construct an LNFN, L, in the following manner. L has $2^n$ hidden units and by definition L has one output unit. The inputs to L are $i_1, ..., i_{2n}$ where $i_1, i_2$ represent $x_1, \lnot x_1$ and so on. Let $\epsilon_b = 1$ for every neuron.\\

Let $h_k$ denote hidden unit k. $h_k$ has the weights $\epsilon_{k,1}, ..., \epsilon_{k,2n}$, where $\epsilon_{k, m}$ represents input $i_m$'s relevance to the output of $h_k$. Similarly the output unit $o$ has weights $\mu_1, .., \mu_{2^n}$ where $\mu_m$ represents the relevance of $h_m$ to the output of $o$.\\

Assume L is a DNF Network. Starting from row one of the table T, to row $2^n$. If row $a$ corresponds to False then set $\mu_a = 1$ (i.e. hidden node $a$ is irrelevant), otherwise the row corresponds to True, then $\mu_a = Z$, where Z is a value close to 0 (any weight for a Noisy neuron cant be exactly 0). For each $\epsilon_{a, m}$ if the corresponding literal occurs in row $a$ of the truth table then $\epsilon_{a, m} = Z$ other wise $\epsilon_{a, m} = 1$.\\

\textbf{Claim:} For some assignment to the atoms of B, $x_1 = v_1, ..., x_n = v_n$ where $v_i \in \{0, 1\}$. Then $L(i_1, ..., i_{2n}) = B(x_1, ..., x_n)$.\\

Assume $B(x_1, ..., x_n) = 1$ for the assignment $x_1 = v_1, ..., x_n = v_n$ corresponding to row $a$ of T. Then if $i_k$ is not considered in row $a$ then $\epsilon_{a,k} = 1$ and if it is present then $i_k = 1$. The output of $h_a$ is given by 

\begin{align*}
&= \prod \epsilon_{a, m}^{1 - i_m}\\
&= Z^{\sum_{i_k = 1}(1 - i_k)}\\
&= Z^0
\end{align*}
Demonstrating that  $\lim_{Z \to 0} Out(h_a) = \lim_{Z \to 0} Z^0 = 1$. Consider the activation of $o$, it is known that $\mu_a = Z$ consequently $\lim_{Z \to 0} \mu_a^{h_a} = \lim_{Z \to 0} Z^1 = 0$, therefore

\begin{align}
\lim_{Z \to 0} Out(o) &= 1 - \prod_{m=1}^{2^n} \mu_m ^{h_m}\\
&= 1 - 0 = 1
\end{align} 

Therefore $L(i_1, ..., i_{2n}) = 1$. Alternatively if $B(x_1, ..., x_n) = 0$ then no hidden neuron will have activation $1$, this can be demonstrated by considering that any relevant neuron (i.e. corresponding $\mu \neq 1$) will have some input weight pair of $i_m$ $\epsilon_m$ such that $\epsilon_m^{i_m} = 0$. Consequently it can be said that for all $m$ $\mu_m^{h_m} = \mu_m^{0} = 1$, therefore the output unit will give $0$, as required.

Now assume that L is a CNF Network. The weights can be assigned in the same manner as before, except rather than considering the rows that correspond to True the negation of the rows corresponding to False are used. If a row $a$ corresponds to True then $\mu_a = 1$, otherwise $\mu_a = Z$ and for any literal present in the row then the input to L which corresponds to the negated literal has weight $Z$, all other weights are $1$.\\

\textbf{Claim:} For some assignment to the atoms of B, $x_1 = v_1, ..., x_n = v_n$ where $v_i \in \{0, 1\}$. Then $L(i_1, ..., i_{2n}) = B(x_1, ..., x_n)$.\\

In this configuration it must be shown that every hidden neuron fires when the network is presented with a variable assignment which corresponds to True and there is always at least one neuron which does not fire when the assignment corresponds to False. Assume for a contradiction that for a given assignment $B(x_1, ..., x_n) = 1$ but $L(i_1, ..., i_{2n}) = 0$. Then there is at least one hidden neuron which does not fire. Let $h_a$ be such a neuron. Consequently for any input weight combination which is relevant $\epsilon_{a,m}^{i_m} = 1$, so $i_m = 0$ for any relevant input. Let $i_{r_1}, ..., i_{r_k}$ be the relevant inputs then $i_{r_1} \lor ... \lor i_{r_k} = False$, so $\lnot(\lnot i_{r_1} \land ... \land \lnot i_{r_k}) = False$, a contradiction as then $B(x_1, ..., x_n)$ would be False.

Now assume for a contradiction $B(x_1, ..., x_n) = 0$ but $L(i_1, ..., i_{2n}) = 1$. Then there exists some $h_a$ with output $1$ where it should be $0$. Consequently there exists at least one input/weight pair with $\epsilon_{a,m}^{i_m} = 1$ that should be $0$. Let $i_{r_1}, ..., i_{r_k}$ be all the relevant inputs, at least one relevant input is present $i_r$. Consequently $i_{r_1} \lor ... \lor i_{r_k} = True$, therefore $\lnot(\lnot i_{r_1} \land ... \land \lnot i_{r_k}) = True$, a contradiction as then $B(x_1, ..., x_n)$ is True.\\
\end{proof}

Theorem \ref{thm:upper-bound-hidden-units} provides justification for using $2^n$ hidden units, it guarantees that there at least exists an assignment of weights yielding a network that can correctly classify each item in the truth table.

\section{Noisy Gate Parametrisation} \label{sec:real-noisy-parametrisation}
The parametrisation of Noisy gates require weight clipping, an expensive operation. A new parametrisation is derived that implicitly clips the weights. Consider that $\epsilon \in (0, 1]$, therefore let $\epsilon_i = \sigma(w_i)$, these $w_i$'s can be trained without any clipping, after training the original $\epsilon_i$'s can be recovered.\\

Now these weights must be substituted into the Noisy activation. Consider the Noisy-OR activation.

\begin{align*}
a_{or}(X) &= 1 - \prod^p_{i=1}(\epsilon_i^{x_i}) \cdot \epsilon_b\\
&= 1 - \prod^p_{i=1}(\sigma(w_i)^{x_i}) \cdot \sigma(b)\\
&= 1 - \prod^p_{i=1}((\frac{1}{1 + e^{-w_i}})^{x_i}) \cdot \frac{1}{1 + e^{-b}}\\
&= 1 - \prod^p_{i=1}((1 + e^{-w_i})^{-x_i}) \cdot (1 + e^{-w_i})^{-1}\\
&= 1 - e^{\sum^p_{i=1} -x_i \cdot ln(1 + e^{-w_i}) - ln(1 + e^{-b})} \\
&Let\ w_i^{'} = ln(1 + e^{-w_i}),\ b^{'} = ln(1 + e^{-b})\\
&= 1 - e^{-(W^{'} \cdot X + b^{'})}
\end{align*}

From a similar derivation we get the activation for a Noisy-AND.

\begin{align*}
a_{and}(X) &= \prod_{p}^{i=1} (\epsilon_i^{1 - x_i}) \cdot \epsilon_b\\
&= \prod_{p}^{i=1} (\sigma(w_i)^{1 - x_i}) \cdot \sigma(w_b)\\
&= e^{\sum^p_{i=1} -(1 - x_i) \cdot ln(1 + e^{-w_i}) - ln(1 + e^{-b})} \\
&= e^{-(W^{'} \cdot (1 - X) + b^{'})}
\end{align*}

Concisely giving equations \ref{equ:real-noisy-and-activation}, \ref{equ:real-noisy-or-activation}

\begin{align}
a_{and}(X) &= e^{-(W^{'} \cdot (1 - X) + b^{'})} \label{equ:real-noisy-and-activation}\\
a_{or}(X)&= 1 - e^{-(W^{'} \cdot X + b^{'})} \label{equ:real-noisy-or-activation}
\end{align}

The function taking $w_i$ to $w_i^{'}$ is the soft ReLU function which is performing a soft clipping on the $w_i$'s. 

\section{Training LNF Networks}
Using equations \ref{equ:real-noisy-or-activation} and \ref{equ:real-noisy-and-activation} for the Noisy-OR, Noisy-AND activations retrospectively allows LNFNs to be trained without the need to clip the weights.\\

Training the networks on all input patterns at the same time lead to poor learning, whereas training on a single example at a time had significantly better results. The ADAM Optimizer is the learning algorithm used, firstly for the convenience of an adaptive learning rate but also because it includes the advantages of RMSProp which works well with on-line (single-example) learning \cite{kingma2014adam}, which LNFNs respond well to.\\

Preliminary testing showed that LNFN's are able to learn good classifiers on boolean gates, i.e. NOT, AND, NOR, NAND, XOR and Implies. It is also possible to inspect the trained weights and see that the networks have learnt the correct CNF or DNF representation.

\section{LNF Network Performance}
How do LNFNs perform against standard perceptron networks which we know to be universal function approximators. Two different perceptron networks will be used as a benchmark

\begin{enumerate}
	\item One will have the same configuration as the LNFNs, i.e. $2^n$ hidden neurons.
	\item The other has two hidden layers, both with N neurons.
\end{enumerate}

The testing will consist of selecting 5 random boolean expressions for $2 \leq n \leq 9$ and training each network 5 times, each with random initial conditions. Figure \ref{fig:peformance-comparason-all} shows a comparison between all 4 of the networks and figure \ref{fig:peformance-comparason-cnfdnf} shows just the LNFN's.

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.8\textwidth}
    \includegraphics[width=\textwidth]{All-Peformance-Comparason.png}
    \caption{}
    \label{fig:peformance-comparason-all}
  \end{minipage}
  \hfill
\end{figure}

\begin{figure}[H]
  \centering
  \begin{minipage}[b]{0.8\textwidth}
    \includegraphics[width=\textwidth]{CNFvsDNF.png}
    \caption{}
    \label{fig:peformance-comparason-cnfdnf}
  \end{minipage}
  \hfill
\end{figure}

Figure  \ref{fig:peformance-comparason-all} shows that neither of the perceptron networks perform as well as the LNF Networks as $n$ increases. Figure  \ref{fig:peformance-comparason-cnfdnf} shows on average there are no statistically significant differences between the CNF or DNF networks. What is not present in Figure \ref{fig:peformance-comparason-cnfdnf} is that sometimes the CNF network consistently out performs the DNF and visa versa, theoretically both should be able to learn any boolean expression.

What causes some expressions to be harder to learn for one type of LNFN compared to another? 

\section{LNF Network Generalization} \label{sec:lnfn-generalization}
These networks are able to perform as well as standard perceptron networks but so far they have been getting the complete set of data, in practice this will almost never be the case. Standard ANN's are widely used because of their ability to generalize, for LNFN's to be useful they must also be able to generalize.

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.8\textwidth}
		\includegraphics[width=\textwidth]{6-generalization.png}
		\caption{}
		\label{fig:generalization-peformance-6}
	\end{minipage}
	\hfill
\end{figure}

Figure \ref{fig:generalization-peformance-6} shows a comparison between the generalization ability of CNF, DNF and Perceptron networks. The graph shows the performance over all training data when successively removing elements from the training set. It demonstrates that the CNF and DNF networks generalize as well as the perceptron networks when the boolean formula has 6 inputs, this trend continues as n increases up to 9.

\section{LNF Network Rule Extraction} \label{sec:lnfn-rule-extraction}
Given the logical nature of LNFNs, is it possible to extract boolean rules. Consider the weights for a logical neuron $W = \{w_1, ..., w_n\}$, These can be converted to $\epsilon_i = \sigma(w_i)$ where $\epsilon_i \in [0, 1]$ and represents the relevance input $x_i$ has on the neurons output.\\

To extract meaningful rules from the network using $\{ \epsilon_1, ..., \epsilon_n \}$ it is important that at the conclusion of training each $\epsilon_i \approxeq 1$ or $\epsilon_i \approxeq 0$. If this is the case then it is possible to interpret the neuron as a purely logical function, say that the neuron in question was a Noisy-OR, then the neuron can be seen as performing a logical OR on all the inputs with corresponding $\epsilon \approxeq 0$.\\

Conjecture \ref{conj:lnfn-approach-binary} is the foundation of the following rule extraction algorithm, it was derived from experimental evidence by training LNFNs over complete truth tables and inspecting the weights. Ideally Conjecture \ref{conj:lnfn-approach-binary} would be proved, but that is out of scope for this report.

\begin{conjecture}
	For an LNFN network trained on a binary classification problem with boolean inputs, as the loss approaches 0  (i.e. the correct CNF or DNF has been found) the weights $\{ w_1, ..., w_n \}$ approach $\infty$ or $-\infty$, consequently each $\epsilon_i$ approaches 0 or 1.
	\label{conj:lnfn-approach-binary}
\end{conjecture}

The Algorithm displayed in figure \ref{alg:rule-extraction} extracts rules from CNFNs, it takes the output weights (ow) and hidden weights (hw) as input and outputs the a boolean expression. A similar algorithm can be derived for DNFNs, it is omitted but can be obtained by simply switching the logical operations around.

\begin{figure}[H]
	\begin{lstlisting}[mathescape=true]
atoms = $\{ x_1, \lnot x_1, ... x_n, \lnot x_n, \}$
	
function extractRulesCNFN(ow, hw)
  ow $= \sigma($ow$)$
  hw $= \sigma($hw$)$
  relvHidden = [hw[i] where ow[i] := 0]
		
  and = And([])
    for weights in relvHidden
      or = Or([atoms[i] where weights[i] := 0])
      and.add(or)
		
  return and
	\end{lstlisting}
	\caption{Rule Extraction Algorithm (for CNFN)}
	\label{alg:rule-extraction}
\end{figure}

In practice many clauses in the extracted expression contain redundant terms, i.e. clauses that are a tautology or a duplicate of another, filtering these out is not an expensive operation.\\

Section \ref{sec:lnfn-generalization} discusses the generalization capabilities of LNFNs compared to MLPNs and shows that they are statistically equivalent. How does training over incomplete truth tables effect the generalization of extracted rules and what factors could influence this?\\

Consider $B$ to be the set of all boolean problems with $n$ inputs. What is the cardinality of $B$, there are $2^n$ rows in the truth table and $2^{2^n}$ ways to assign true/false values to these rows, each way corresponding to a different boolean function, consequently $|B| = 2^{2^n}$. So consider some $b \in B$ represented by $2^n$ rows of a truth table, removing one row from the training data means there are now two possible functions that could be learnt, one where the removed row corresponds to true and the other to false. As more rows are removed this problem is compounded, if $m$ rows are taken then there are $2^m$ possible functions.\\

When constructing a CNF or DNF from a truth table as discussed in Section \ref{subsec:construct-cnfdnf}, in the case of CNF only the rows corresponding to false are considered and for the DNF only rows corresponding to true. Despite the fact that if $m$ rows are removed from the training set then there are $2^m$ possible functions that could represent the partial truth table, learning the CNF and DNF may alleviate some of the issues caused by it, another possibility is to combine the CNF and DNF formulas to create a better rule set.\\

Figure \ref{fig:cnf-descrete-generalizatiion} shows how the rule set of an CNFN generalizes as examples are removed, figure \ref{fig:dnf-descrete-generalizatiion} shows the same but for DNFNs.

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{cnf-descrete-generalization.png}
		\caption{}
		\label{fig:cnf-descrete-generalizatiion}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{dnf-descrete-generalization.png}
		\caption{}
		\label{fig:dnf-descrete-generalizatiion}
	\end{minipage}
	\hfill
\end{figure}

In figures \ref{fig:cnf-descrete-generalizatiion} \& \ref{fig:dnf-descrete-generalizatiion} the training examples which are removed get randomly selected, how is the performance effected if the removed examples are chosen more carefully. In the next experiment only examples corresponding to false are removed and the resultant training set is given to a DNFN.

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{cnf-descrete-generalization-partial.png}
		\caption{}
		\label{fig:cnf-descrete-generalizatiion-partial}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{dnf-descrete-generalization-partial.png}
		\caption{}
		\label{fig:dnf-descrete-generalizatiion-partial}
	\end{minipage}
	\hfill
\end{figure}

Figures \ref{fig:cnf-descrete-generalizatiion-partial} \& \ref{fig:dnf-descrete-generalizatiion-partial} demontrait CNFNs and DNFNs trainined over partial data retrospectively. In the case of CNFNs only true entries of the truth table are removed and for the DNFNs only false entries. For the most part the minimum line is lower however the average line is roughly the same, this would indicate that while the choice of examples removed has an effect  initial conditions have a more significant influence on the generalization performance.

\chapter{Investigation of Logical Normal Form Networks} \label{C:investigation-of-lnfns}
\section{Developing Applications} \label{sec:developing-applications}
The LNFNs presented in Chapter \ref{C:foundation-of-lnfns} have very limited applications, the class of problems they can be applied to all have the form of binary classification where the features are Boolean. To make LNFNs more useful they need a broader scope.

\subsection{Multi-class Classification}
How can LNFNs be developed] to support classification of more than two classes. If attempting to learn a Multi-class Classification problem with $n$ distinct classes $c_1, ..., c_n$, then $n$ LNFNs would have to be trained, each a learning a binary classification problem $c_i$, $\lnot c_i$.\\

An intuitive way to extend LNFNs to support Multi-class Classification is to add more output neurons and use One-hot encoding. If we have 3 classes then $100$, $010$ and $001$ represent class 1, 2 and 3 retrospectively, then the LNFN would have 3 output neurons, each representing a bit in the One-hot encoded string.\\

\begin{definition}
	The structure of an LNFN to solve an $k$ class classification problem where each instance is described by n features has $2n$ inputs, $2^n$ hidden units and $k$ output units. A final Softmax layer is added, as is convention for these types of problems.
\end{definition}

A simple problem which lends its self naturally to this is the \text{Lenses} problem \cite{Lichman:2013}, a three class classification problem, each instance has 4 features, 3 of which are binary and the other has three possible values. This problem can be easily converted into one which can be used with an LNFN, each new problem instance will have 6 features, the three binary remain the same and the categorical one is expanded into 3.\\

How does an LNFN network perform when compared to a MLPN on the Lenses problem. The performance of the two classifiers will be compared using Leave-One-Out (LOE) Cross-Validation. The structure of the MLPN only differs in the number of hidden layers/units, there are two hidden layers, one with $2 \cdot n$ hidden units and the other with $n$

\begin{table}[H]
	\begin{center}
		\begin{tabular}{| c | c | c |}
			\hline
			& Error (Cross Entropy) & Confidence Interval (95\%) \\
			\hline
			CNF Net & 6.663 & (6.468, 7.198) \\
			\hline
			DNF Net & 6.660 & (6.468, 7.197) \\
			\hline
			PCEP Net & 6.751 & (6.468, 7.997) \\
			\hline
		\end{tabular}
	\end{center}
	\caption{}
	\label{tab:lenses-peformance-comp}
\end{table}

Table \ref{tab:lenses-peformance-comp} demonstrates that the CNF \& DNF Networks perform comparably to an MLPN as the confidence intervals for the error overlap.\\ 

Now that the LNFN network has three output neurons it should be possible to extract three rules describing each of the classes. Consider that each problem instance is of the following form $\{a, b, c, d, e, f\}$ where $a,b,c,d,e,f$ are all atoms. The following rules can be extracted from a CNFN when trained over the complete Lenses data, any duplicate clause or Tautology has been filtered out, the resultant extracted formula has also been manually simplified (so they can be displayed and understood better).

\begin{itemize}
	\item \text{Class 1:} $(a \lor b \lor e) \land (a \lor \lnot d) \land (c \lor e) \land f$
	\item \text{Class 2:} $(a \lor b \lor \lnot c \lor d) \land \lnot e \land f$
	\item \text{Class 3:} $(\lnot a \lor b \lor c \lor \lnot f) \land (a \lor \lnot d \lor e \lor \lnot f) \land (\lnot b \lor c \lor d \lor \lnot f) \land (d \lor \lnot e \lor \lnot f)$
\end{itemize}

Immediately it is possible to find useful information about this problem that was not obvious before, namely $\lnot f = True \implies $ Class 3. Table \ref{tab:rule-classification-lenses-cnf} shows these rules applied to all the problem instances in the Lenses data set, it demonstrates that these extracted rules are able to fully describe the Lenses problem.\\

The DNFN might be more applicable as the rules will be more insightful, given its structure as an OR of ANDs.\\

\begin{itemize}
	\item \text{Class 1:} $(a \land \lnot b \land \lnot c \land e \land f) \lor (\lnot a \land \lnot d \land e \land f)$
	\item \text{Class 2:} $(\lnot c \land \lnot e \land f) \lor (c \land d \land \lnot e \land f)$
	\item \text{Class 3:} $(\lnot a \land \lnot b \land c \land \lnot d) \lor (\lnot a \land d \land e) \lor \lnot f$
\end{itemize}

These DNF formula do not correspond to the CNF give above but this is to be expected, despite the fact that collectivity all the problem instances span the space of features once they have been converted to the boolean feature form this fact is no longer true.    there is a way to combine the knowledge from both the CNF and DNF formula to create resultant formula which perform better.\\

One possible issue that could arrive here is that the CNFN is now attempting to learn three CNF expressions with the same number of hidden neurons. The fact that meaningful rules where able to be extracted in this case could of been a coincidence, intuitively if we are learning a problem with $k$ classes then we could need $k \cdot 2^n$ hidden neurons which is becoming impractical.

\subsection{Features with Continuous Domains}
The inputs to an LNFN are allowed to be continuous but must be in the rage $[0, 1]$, would it still be possible to extract meaningful rules from the network if the inputs are continuous? Here there are two things to investigate. What can be achieved by training an LNFN on problems with continuous features? Secondly what can be achieved by discretizing the continuous inputs and then using an LNFN to learn this new boolean problem. A simple benchmark to use is the Iris problem \cite{Lichman:2013}

\subsubsection{LNFNs and Continuous Features} \label{subsubsec:lnfns-cont-features}
If the features are continuous it no longer makes sense to extract rules but it could still be possible to see what inputs are considered in making a decision about the class. When training an LNFN on the Iris problem (over all problem instances) the network converges to a solution with a loss of $96.932$, poor performance when compared to a perceptron network that can achieve an accuracy of $0.0$. \\

Inspecting the class prediction of each reveals that for a problem instance that has a true class of Iris-virginica or Iris-versicolor then LNFN sometimes predicts multiple classes, this leads to the belief that these networks have issues with learning problems that are not linearly separable as the two classes which an LNFN has trouble differentiating between are not linearly separable.\\

The LNFN is able to learn XOR so not all problems which are not linearly separable are out of reach of LNFNs. Section \ref{subsubsec:lnfns-desc-cont-features} investigates whether it is possible to descretize the variables in such a way that makes this problem learnable by LNFNs.


\subsubsection{LNFNs and Discretized Continuous Features} \label{subsubsec:lnfns-desc-cont-features}
There are a number of method for discretizing continuous features, there are many algorithms for performing such an operation on some data \cite{liu2002discretization}, to many for all to be tested. Two simple methods for descretization are Equal width or frequency binning, these methods are very naive and prone to outliers, also the number of bins must be chosen before hand so this requires experimentation. A supervised partitioning method will also be tested, namely Recursive Minimal Entropy Partitioning.\\

Results from training LNFNs with descretized data results in the same issue as before, the network has problems with classifying classes that are not linearly separable.

\subsubsection{Discussion of Application to Continuous Domains}
The ideas explored in Sections \ref{subsubsec:lnfns-cont-features} \& \ref{subsubsec:lnfns-desc-cont-features} demonstrate that applying LNFNs to problems with continuous inputs is not viable.\\

Another thing to consider is that it becomes difficult to justify what the input features mean in the context of Noisy gates. In terms of the iris problem each feature represents a length, sure its possible to normalize the features to $[0,1]$ and train an LNFN anyway but essentially the lengths are being forced to be some sort of truth value.

\chapter{Logical Neural Networks}
The algorithm shown in Figure \ref{alg:rule-extraction} presents a method for extracting rules from LNFNs, there is no reason why this algorithm cant be applied to Logical Neural Networks (LNNs) \cite{LearningLogicalActivations} which can take on any configuration.\\

An issue with LNFNs is that the number of hidden units allows for the possibility to memorise the input data, using LNNs consisting of more layers with a smaller width could force the learning of better rules.\\

Before investigating this there are two key issues caused by removing the restrictions imposed by the LNFN definition (Definition \ref{def:lnfn}) which must be addressed 

\begin{enumerate}
	\item Noisy neurons do not have the capacity to consider the presence of the negation of an input. This was a problem for LNFNs as well, however given that only the negations of atoms need to be considered to learn a CNF or DNF it was easily fixed by presenting the network with each atom and its negation. The problem can not be solved so easily for LNNs. A boolean formula can not always be represented by only AND and OR gate, i.e the set of operations $\{AND, OR\}$ is not functionally complete. 
	
	\item Another problem faced by LLNs that are not restricted to be ether a CNFN or DNFN is that the structure of the network will have a higher impact on whether the problem can be learnt. 
\end{enumerate}

Despite the issues outlined above being able to implement LNNs with layers that are not required to be $2^n$ hidden units wide would be a lot more practical, in practice the number of features could be upwards of 100 and $2^{100}$ is a huge number of hidden neurons.\\

Resolving Issue 1 involves making our operation set functionally complete, this requires the $NOT$ operation. There are two ways to include the $NOT$ operation in the LNNs, one is to simply augment the inputs to each layer appending so it receives the input and its negation, a more complicated but more elegant solution is to derive a parametrisation of Noisy gates which can learn to negate inputs. However both these have no way to enforce mutual exclusivity between an input and its negation.\\

An LNN, consisting of Noisy-OR hidden units and Noisy-AND outputs, with without any modification was shown to achieve a classification accuracy of $8.67$ on the MNIST data set, it was also conjectured that using an LNN with an AND layer followed by OR does not learn well \cite{LearningLogicalActivations}. 


Before running any tests on the MNIST dataset LNNs with and without not operations will be compared.

\section{Modified Logical Neural Network Structure} \label{sec:modified-lnn-structure}
\subsection{Connections Between Layers \& Parameters}
Figure \ref{fig:modified-lnn-structure} provides a new structure for the connections between the lectures.

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.9\textwidth}
		\includegraphics[width=\textwidth]{Modified-LNN-Structure.png}
		\caption{}
		\label{fig:modified-lnn-structure}
	\end{minipage}
	\hfill
\end{figure}


Figure \ref{fig:modified-lnn-structure} shows the new LNN structure which includes an implicit NOT operation. The input to each layer consists of the true output of the previous plus the negated output from the last layer. If a layer has 20 outputs then there are 40 inputs to the following layer.\\

\subsection{Softmax Output Layer}
The old LNN structure does not include a Softmax layer for the output. On the one hand a traditional Softmax layer would not be effective as the neuron outputs are limited to the range $[0,1]$, so a different method must be employed to ensure the mutual exclusivity of the classes.\\

Consider the following vector of probabilities $P = \{p_1, ..., p_n\}$ where $p_i$ is the probability the given example belongs to class $i$. Then define $p_i^{'} = \frac{p_i}{\sum_j p_j}$, performing this action to generate a vector $P^{'}$ where each of the classes are mutually exclusive.


\section{Weight Initialization}

This new structure is a super set of LNFNs, it should be possible to create and train an CNFN or DNFN with the modified LNN, however this is not the case. There are many more weights in the network now, this specifically causes an issue when the output neuron is a Noisy-AND, the current weight initialization technequic causes the output to saturate and nothing is able to be learnt when there exist to many neurons in the network. Before proceeding to experiment with the modified LNN a method to initialize the weights must be developed.\\

\paragraph{Deriving a Distribution For The Weights}
Before a weight initialization algorithm can be derived good learning conditions must be identified. Ideally the output of each neuron would be varied for each training example, i.e. $y \sim U(0,1)$. Each training example $X = \{x_1, ..., x_n\}$ has each component $x_i \in (0,1]$, it will be assumed that $x_i \sim U(0,1)$. If the input vectors and output of each neuron are both distributed $U(0,1)$ then the input to any layer is distributed $U(0,1)$, based on this fact it can be argued that the weight initialization is the same for both Noisy-OR and Noisy-AND. Recall the activations for Noisy neurons\\

\begin{align*}
	a_{AND}(X) &= e^{-(w_1(1 - x_1) + ... + w_n(1 - x_n))}\\
	a_{OR}(X) &= 1 - e^{-(w_1x_1 + ... + w_nx_n)}
\end{align*}

Consider a random variable $g$, if $g \sim U(0,1)$ then $1 - g \sim U(0,1)$ also holds. Consequently, if $x_i \sim U(0,1)$ then $1 - x_i ~ U(0,1)$, also $e^{-z} \sim U(0,1)$ then $1 - e^{-z} ~ U(0,1)$. It is therefore enough to consider $e^{-(w_1x_1 + ... + w_nx_n)}$ when deriving the initialization method for each $w_i$.\\

Strictly speaking each $w_i = \log(1 + e^{w^{'}_i})$ (as derived in Section \ref{sec:real-noisy-parametrisation}) however for the purposes of this initialisation derivation it will be assumed that $w_i \in (0 \infty]$.\\

Given that $y = e^{-z} ~ U(0,1)$, a first step is to determine the distribution of $z$.

\begin{theorem}
	If $y \sim U(0,1)$ and $y = e^{-z}$, then $z \sim exp(\lambda = 1)$
\end{theorem}
\begin{proof}
	Consider that $y = e^{-z}$ can be re written as $z = -\log(y)$.
	
	\begin{align*}
		F(z) &= P(Z < z)\\
		&= P(-\log(Y) < z)\\
		&= P(\frac{1}{Y} < e^{-z})\\
		&= P(Y \geq e^{-z})\\
		&= 1 - P(Y \leq e^{-z})\\
		&= 1 - \int_{0}^{e^{-z}} f(y) dy\\
		&= 1 - \int_{0}^{e^{-z}} 1 dy\\
		&= 1 - e^{-z}
	\end{align*}
	
	Therefore $F(z) = 1 - e^{-\lambda z}$ where $\lambda = 1$. Consequently $z \sim exp(\lambda = 1)$
\end{proof}

The problem has now been reduced to find how $w_i$ is distributed given that $z \sim exp(\lambda = 1)$ and $x_i \sim U(0,1)$. The approach taken is to find $E[w_i]$ and $var(w_i)$.

\begin{align*}
	E[z] &= E[w_1x_1 + \cdot \cdot \cdot + w_nx_n]\\
	&= E[w_1x_1] + \cdot \cdot \cdot + E[w_nx_n]\ (independence)\\
	&= E[w_1]E[x_1] + \cdot \cdot \cdot + E[w_n]E[x_n]\\
	&= n \cdot E[w_i]E[x_i]\ (i.i.d)\\
	&= n \cdot E[w_i] \cdot \frac{1}{2}\\
	1 &= \frac{n}{2} \cdot E[w_i]\\
	E[w_i] &= \frac{2}{n}
\end{align*}

\begin{align*}
	var(z) &= var(w_1x_1 + \cdot \cdot \cdot + w_nx_n)\\
	&= var(w_1x_1) + \cdot \cdot \cdot + car(w_nx_n)\\
\end{align*}

\begin{align*}
	var(w_ix_i) &= (E[w_i])^2var(x_i) + (E[x_i])^2var(w_i) + var(w_i)var(x_i)\\
	&= \frac{4}{n^2} \cdot \frac{1}{2} + \frac{1}{4} \cdot var(w_i) + var(w_i) \cdot \frac{1}{12}\\
	&= \frac{1}{3 n^2} + \frac{1}{3}var(w_i)
\end{align*}

Consequently the variance can be found by the following

\begin{align*}
	1 &= n \cdot \big[\frac{1}{3 n^2} + \frac{1}{3}var(w_i)\big]\\
	3 &= \frac{1}{n} + n \cdot var(w_i)\\
	3n &= n^2 var(w_i)\\
	var(w_i) &= \frac{3}{n}
\end{align*}

From the above arguments $E[w_i] = \frac{2}{n}$ and $var(w_i) = \frac{3}{n}$. These values need to be fitted to a distribution that weights can be sampled from. Based on our initial assumptions this distribution must also generate values in the interval $[0, \infty]$. Potential distributions are  Beta Prime, Log Normal.

\paragraph{Fitting To Beta Prime} Consider the Beta Prime distribution which as two parameters $\alpha$ and $\beta$ which must be solved for given the mean and variance.

\begin{align*}
	E[w_i] &= \frac{2}{n}\\
	&= \frac{\alpha}{\beta - 1}\\
	\alpha &= (\beta - 1) \cdot \frac{2}{n}
\end{align*}

\begin{align*}
	var(w_i) &= \frac{3}{n}\\
	&= \frac{\alpha (\alpha + \beta - 1)}{(\beta - 2)(\beta - 1)^2}\\
	&= \frac{(\beta - 1) \cdot \frac{2}{n} ((\beta - 1) \cdot \frac{2}{n} + \beta - 1)}{(\beta - 2)(\beta - 1)^2}\\
	&= \frac{\frac{2}{n} ((\frac{2}{n} + 1))}{(\beta - 2)}\\
	(\beta - 2) &= \frac{4 + 2n}{3n}\\
	\beta &= \frac{4 + 8n}{3n}
\end{align*}
Finally giving $\alpha = \frac{10n + 8}{3n^2}$.

\paragraph{Fitting To Log Normal}

\textbf{ADD MATH HERE}
\begin{align*}
	\sigma^2 &= \log (4(4 + 3n)))\\
	\mu &= -\frac{1}{2}\log (n^2(4 + 3n))
\end{align*}


\paragraph{Weight Initialization for LNNs}
Through experiments the weights sampled from a Log Normal distribution perform better than when sampled from a Beta Prime distribution. The algorithm for weight initialization can now be given but first consider that each weight that is sampled from the Log Normal distribution has the following property $w \sim LN,\ w = f(w^{'})$ where $w^{'}$ can be any real value, consequently to obtain the initial weights each $w$ must be inversely transformed back to the space of all real numbers.

\begin{figure}[H]
	\begin{lstlisting}[mathescape=true]
  function constructWeights(size):
    $w_i \sim LN$ (for i = 0 to size)
    return $f^{-1}(\{w_0, ...\})$
	\end{lstlisting}
	\caption{Weight Initialization Algorithm for LNNs}
	\label{alg:lnn-initlization}
\end{figure}

\section{Conjectures Concerning Performance of LNNs}

Based on experimental evidence the following Conjectures \ref{conjecture:lnn-stuck} \& \ref{conjecture:lnn-rule-extraction} can be made. Ideally formal arguments would be given, however this is out of scope for this project.

\begin{conjecture}
	If the given network structure is unable to learn the formula it gets stuck.
	\label{conjecture:lnn-stuck}
\end{conjecture}

\begin{conjecture}
	If the loss is small enough then similarly to LNFNs rules can be extracted directly from the weights.
	\label{conjecture:lnn-rule-extraction}
\end{conjecture}

The LNN structure is significant when it comes to the learnability of problems so conjectures \ref{conjecture:lnn-stuck} \& \ref{conjecture:lnn-rule-extraction} allow for determining whether a network configuration is correct.


\chapter{Evaluation Of Logical Neural Networks}
\section{Tic Tac Toe} \label{sec:lnn-ttt}
This problem involves classifying tic-tac-toe endgame boards and determining whether 'x' can win \cite{Lichman:2013}. There are 9 categorical attributes, representing each cell of the board, each cell can have 'x', 'o' or 'b' for blank.\\

This gives a total of 27 attributes, if using an LNFN then the hidden layer would consist of 134217728 neurons, an intractable number, if the computer did not run out of memory then computing the gradients would be very slow.

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.5\textwidth}
		\includegraphics[width=\textwidth]{Tic-Tac-Toe-Net.png}
		\caption{}
		\label{fig:tic-tac-toe-net}
	\end{minipage}
	\hfill
\end{figure}

There are a total of 958 instances, 70\% of which will be used for training, the rest for testing. Using the new LNN with the structure described in Figure \ref{fig:tic-tac-toe-net} is able to achieve an error rate of $0.1\%$ on the training set and $0.3\%$ on the test set. The rules extracted from the network have an error rate of $1.7\%$ on the training set and $6.3\%$ on the test set.\\

What about the performance of the same network but swap around the activations? It was conjectured that networks of this form have poor learning \cite{LearningLogicalActivations} but with these new LNNs it might not be true any more.\\

Changing the configuration so that the AND activation is before the OR obtains a result which is better than the previous. The network has the same performance but the rule set extracted has a $0\%$ error rate on the training set and a $2\%$ error on the test.\\

It can be concluded that the conjecture saying that an LNN with an AND-OR configuration does not learn no longer holds.

\subsection{Evaluation of LNN Rules}
The rule extraction algorithm given in Figure \ref{alg:rule-extraction} is an electic algorithm as this category describes algorithms that examine the network at the unit level but extract rules which represent the network as a whole. The algorithm is certainly not portable as it can only be applied to networks with the LNN architecture.\\

Finally what is the quality of the rules extracted from LNN, this is measured by the Accuracy, Fedelity, Consistency and Comprehensibility \cite{andrews1995survey}.

\begin{enumerate}
	\item \textbf{Accurate:} As demonstrated experimentally the extracted rules are able to generalise well to unseen examples.
	\item \text{Fedelity:} The experiments also show that the rules perform very similar to the network they where extracted from.
	\item \text{Consistency:} \textbf{TEST THIS}
\end{enumerate}

\section{Application to MNIST}
To determine if this new LNN structure can out perform the old it will be tested on the MNIST problem. The old LNN structure was able to achieve a classification accuracy of 8.67\% on a test set.\\

There are 784 features per example and each will lie in the range $[0, 1]$. In Section \ref{sec:developing-applications} it was discussed why these networks should not be applied to continuous domains, however in this case the inputs still make sense, each feature can be interpreted as the probability that the corresponding pixel in the image is white.\\

Given that the features are continuous the rules no longer make sense but it is still possible to visualise the weight to get an understanding of what the network has learnt. A number of different architectures are trained and inspected, below is a list of each along with a justification as to why it could be effective.

\begin{enumerate}
	\item \textbf{OR-AND Original LNN}, this is because the Noisy parametrisation has been changed and while it should be equivalent it is possible that it is not.
	
	\item \textbf{OR-AND Modified LNN}, to compare the modified structure with the old.
	
	\item \textbf{AND-AND Modified LNN}, a logical way to consider a digit is as an AND of features where each feature is an AND of pixels, however a problem with this approach could be that the border of each feature is fuzzy due to the different positioning and shape of the digits.
	
	\item \textbf{AND-OR-AND Modified LNN},
\end{enumerate}

\subsection{OR-AND Modified LNN}
An OR-AND LNN with 30 hidden neurons and the modified structure is able to achieve an 4\% error rate on the training data along with a 4.5\% error rate on the test set, this is a significant improvement on previous results making LNNs a more feasible option.\\

The intepretability of these weights is also something which needs to be assessed. There are two things to consider, first the individual weights but also the combinations of hidden features which make up each digit.\\

Figures \ref{fig:lnn-or-and-neuron-0} \& \ref{fig:lnn-or-and-neuron-9} \& \ref{fig:lnn-or-and-neuron-13} \& \ref{fig:lnn-or-and-neuron-28} show the weights of 4 distinct neurons in the LNN model. Each image, while sparse, is not necessarily any more interpretable, it is not obvious what each represents.

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-OR-AND/Neuron-0.png}
		\caption{}
		\label{fig:lnn-or-and-neuron-0}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-OR-AND/Neuron-9.png}
		\caption{}
		\label{fig:lnn-or-and-neuron-9}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-OR-AND/Neuron-13.png}
		\caption{}
		\label{fig:lnn-or-and-neuron-13}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-OR-AND/Neuron-28.png}
		\caption{}
		\label{fig:lnn-or-and-neuron-28}
	\end{minipage}
	\hfill
\end{figure}

In terms of the pixels considered in deciding between digits, Figures \ref{fig:lnn-or-and-digit-2} \& \ref{fig:lnn-or-and-digit-3} \& \ref{fig:lnn-or-and-digit-5} \& \ref{fig:lnn-or-and-digit-8} show the pixel weightings with respect to digit 2,3,5,8 retrospectively. The results are promising as there are cerntialy noticeable similarity with the digits they are attempting to represent, inpeticular the outlines of each of the digits are certainly present. These visualisations indicate that this trained network is using the outlines of the numbers to make its determination.

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-OR-AND/Digit-2.png}
		\caption{}
		\label{fig:lnn-or-and-digit-2}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-OR-AND/Digit-3.png}
		\caption{}
		\label{fig:lnn-or-and-digit-3}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-OR-AND/Digit-5.png}
		\caption{}
		\label{fig:lnn-or-and-digit-5}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-OR-AND/Digit-8.png}
		\caption{}
		\label{fig:lnn-or-and-digit-8}
	\end{minipage}
	\hfill
\end{figure}

The model is certainly interpretable, however only once the features have been combined do they make sense. In an ideal world each of the individual features would correspond to something which represents part of a digit. In an effort to make the hidden features more interpretable another OR-AND model is learnt but with fewer hidden neurons.

\subsection{AND-OR Modified LNN}
This model required a lower learning rate and thus a longer training time.
\subsection{AND-AND Modified LNN}
Training a Modified LNN with 30 hidden neurons using an AND-AND structure was able to achieve a 3.7\% error rate on the training set and 4.1\% error rate on the test set. Similarly to the OR-AND model the hidden features are sparse but not necessary interpretable, as shown by Figures \ref{fig:lnn-and-and-neuron-0} \& \ref{fig:lnn-and-and-neuron-9} \& \ref{fig:lnn-and-and-neuron-13} \& \ref{fig:lnn-and-and-neuron-28}

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-AND-AND/Neuron-0.png}
		\caption{}
		\label{fig:lnn-and-and-neuron-0}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-AND-AND/Neuron-9.png}
		\caption{}
		\label{fig:lnn-and-and-neuron-9}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-AND-AND/Neuron-13.png}
		\caption{}
		\label{fig:lnn-and-and-neuron-13}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-AND-AND/Neuron-28.png}
		\caption{}
		\label{fig:lnn-and-and-neuron-28}
	\end{minipage}
	\hfill
\end{figure}

Figures \ref{fig:lnn-and-and-digit-2} \& \ref{fig:lnn-and-and-digit-3} \& \ref{fig:lnn-and-and-digit-5} \& \ref{fig:lnn-and-and-digit-8} show the image pixels considered for the output neurons that classify 2,3,5,8 retrospectively. While obtaining a comparable performance to the OR-AND structure these weight patterns are much less interpretable, there is no distinctive shape in any which indicate there corresponding number. The only determination which can be made is that these appear to consider the filling of each digit as opposed to the boundary, which intuitively makes sense, the boundary will be inherently noisy (as each digit is drawn slightly differently) and AND of pixels will be heavily penalised for anything missing. This is evidence that while using ANDs in a first hidden layer might provide good performance it does not lead to interpretable results.

\begin{figure}[H]
	\centering
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-AND-AND/Digit-2.png}
		\caption{}
		\label{fig:lnn-and-and-digit-2}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-AND-AND/Digit-3.png}
		\caption{}
		\label{fig:lnn-and-and-digit-3}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-AND-AND/Digit-5.png}
		\caption{}
		\label{fig:lnn-and-and-digit-5}
	\end{minipage}
	\begin{minipage}[b]{0.45\textwidth}
		\includegraphics[width=\textwidth]{LNN-AND-AND/Digit-8.png}
		\caption{}
		\label{fig:lnn-and-and-digit-8}
	\end{minipage}
	\hfill
\end{figure}

\subsection{Discussion Of Results}

\chapter{Application to *}
\textbf{This will hopefully be LNNs applied to something cool!}