\documentclass{article}

\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}
\usepackage[linesnumbered,ruled]{algorithm2e}

\theoremstyle{definition}
\newtheorem{definition}{Definition}[section]
\newtheorem{theorem}{Theorem}

\title{%
	Logical Neural Networks: Opening the black box\\
	\large Logical Normal Form Network
}
\author{Daniel Braithwaite}

\begin{document}

\maketitle

Given a simple feed foward network using neurons resembleing NAND gates is unable to learn any given boolean formula we switch our interest to something else. We know that any boolean expression can be represented in Conjunctive Normal Form (CNF) or Disjunctive Normal Form (DNF). So it seems prudent to ask wether its possible to construct a feedfoward netural netowrk which can learn these underlying representations. Findings \cite{herrmann1996backpropagation} seem to sugest that it is possible however there is limited justification for some claims and because of this is diffcult to reproduce. We will take this general concept and reproduce the research in an attempt to develop a better understanding.

\section{Noisy Neurons}
During a previous investigation of Logical Neural Networks the concept of Noisy-OR and Noisy-AND neurons where derived \cite{LearningLogicalActivations}. Here we will simply state them as.

\theoremstyle{definition}
\begin{definition}{}
A \textbf{Noisy-OR} neuron is a perceptron with activation $a = 1 - e^{-z}$ where $W$ is our weights, $X$ is our inputs, $b$ is our byas term and $z = WX + b$. We contrain each $w_i \in W$ and b to be in the interval $[0, \inf]$
\end{definition}

\begin{definition}{}
A \textbf{Noisy-OR} neuron is a perceptron with activation $a = e^{-z}$ where $W$ is our weights, $X$ is our inputs, $b$ is our byas term and $z = WX + b$. We contrain each $w_i \in W$ and b to be in the interval $[0, \inf]$
\end{definition}

\section{Logical Normal Form Networks}
A Logical Normal Form Network is a neural net which stasfys one of the folowing definitions

\theoremstyle{definition}
\begin{definition}{CNF-Network}
A \textbf{CNF-Network} is a three layer network where neurons in the hidden layer consist soley of Noisy-OR's and the output layer is a single Noisy-AND. 
\end{definition}

\theoremstyle{definition}
\begin{definition}{DNF-Network}
A \textbf{DNF-Network} is a three layer network where neurons in the hidden layer consist soley of Noisy-AND's and the output layer is a single Noisy-OR. 
\end{definition}

It is worth noteing that CNF and DNF form can have the nots of atoms in there clauses, a simple way to account for this is to double the number of inputs where one represents the atom and the next represents the negation of that atom.

\subsection{Learnability Of Boolean Gates}
Theoretiaclly it makes sense for these networks to be able to learn the CNF and DNF representations of various boolean expressions, however before we attempt arbatary boolean functions we would like to start with something simple, namely expressions such as NOT, AND, NOR, NAND, XOR and IMPLIES. Results of which where promising, we where able to achieve a low error and from inspecting the weights we could see that the networks where infact learning the correct CNF and DNF representations.\\

\subsection{Learnability Of Interesting Expressions}
We now wish to see if we can use these networks to learn more interesting boolean formula, starting with expressions of 3 variables. While we are able to achieve a small error there is now some noise (i.e. small non zero weights for inputs that are irrelevant). While a small ammount of noise is okay and can be pruned out after training we could run into issues if the ammount of noise increases as the number of inputs does. \\

One other interesting observation to be made is that both the CNF and DNF Netoworks have trouble learning the boolean expression $(a\ XOR\ b)\ AND\ c$. They can't achevice an error lower than 1. Indivually we can learn an XOR gate and an AND gate but somehow combining the two results in something which is unlearnable.\\

During the investigation of this a more sinister issue was uncovered, namely that these LNF netowrks are unable to learn boolean expressions of 2 variables when given 3. I.e. we give the network all values for three inputs, a, b and c but we only want to learn $a\ OR\ b$. This is a fundimental issue as in practice most problems will be made up of boolean expressions which dont rely on all inputs. \\

It turns out this problem was not caused by a problem with our LNF Networks but with the weight initilizations, namely they where currently initilised to 0, changing the weights to be randomly distrubuted over the interval [0,1] fixed this problem and allows the LNF Networks to solve all attempted problems so far, along with there weight representations being intepretable. However the networks seem quite sensitive to there intial conditions, an investgation of best ways to initilise LNF networks would be prudent.

\section{LNF Network Peformance}
We wish to compare the CNF and DNF networks against each other but also against standard perceptron networks. We will take 5 randomly choosen boolean expressions of n inputs for n between 2 and 10. For each we will train CNF, DNF and Perceptron networks 5 separate times and use the datapoints to peform significance tests between the three.

\section{LNF Network Generalization}
Say we are trying to learn a boolean function of $n$ inputs, we know for a fact there are $2^n$ total input patterns in total for this boolean function. We also know there are $2^n$ total possible boolean functions with $n$ inputs. So an important question to ask is once we start to remove some of the traning examples what happens to our network accuracy? The usefulness of standard neural networks is in part because they are able to take a sample of the total data set and then generalise well to unseen examples, can we achieve the same with LNF networks?\\

We will take one boolean expression with $n = 4$ and slowley remove from the pool of traning examples, training a fresh network each time. This will allow us to see a trend of network accuracy as we remove more and more from the training pool.

\section{LNF Network Rule Extraction}
The motivation for this project is to beable to train these networks and esentially extract rules from them. How ever as the networks get larger we see that the networks pickup noise so we must investigate how to remove this noise and then extract rules from the network

\subsection{LNF Network Pruning}
A nice property of LNF Networks is that there weights directly represent how relevant they are to the network. The larger the weight the more important that perticular input is. Our goal here is to find weights less than some threshold and set them to 0, removing noise and allowing us to uncover the CNF or DNF expression (or something close to it).

The question now becomes how to we decide on a threshold. We could investigate trained networks for which we know what the respresentation should look like and decide on what an approaite threshold should be. From my investigation I have found that in allcases weights less than 1 represent noise, so this would be a good place to start. We will call this method relevance pruning.

\subsubsection{Relevance Pruning}
\begin{algorithm}
\SetKwInOut{Input}{Input}
\SetKwInOut{Output}{Output}

\underline{Function: Relevance Prune} $(hidden,output,threshold)$\\
\Input{The weights for the hidden nodes and output nodes finally a threshold for the weights}
\Output{Pruned weights}

$H \gets \varnothing$
$O \gets \varnothing$

\For{$i \gets\ 1\ to\ |H|$}{
\If{$output_i < threshold$}{
$H_{i} \gets {0,...,0}$
$O_i \gets 0$
}
\Else{
$H_{i} \gets PRUNE(hidden_{i}, threshold)$   \tcc{PRUNE simply sets any value less than threshold to 0}
$O_i \gets output_i$
}

\Return{H, O}
}
\caption{Algorythm for peforming Relevance Pruning on some LNF Network}
\end{algorithm}

It should be noted that this algorythm dosnt account for byases being in the weight vectors, if they are some indicies will need to be shifted

\medskip
\bibliographystyle{acm}
\bibliography{bibliography}
\end{document}